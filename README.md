# Long Input Summarization bibliography

A curated list of resources for Long Input Summarization.  
Under Yale Advanced NLP Assignment

## Contributing
Please feel free to email Ziming Mao (ziming.mao@yale.edu).

## Table of Contents

- [Papers](#papers)
  - [Sparse Attention](#sparse-attention)
  - [Extract-then-generate](#extract-then-generate)
  - [Divide-and-conquer](#divide-and-conquer)
  - [Hierarchical Models](#hierarchical-models)
- [Datasets](#datasets)
  - [Documents](#documents)
  - [Dialogues](#dialogues)

## Papers
### Sparse Attention

* Longformer: The Long-Document Transformer [[paper](https://arxiv.org/abs/2004.05150)]
* Big Bird: Transformers for Longer Sequences [[paper](https://arxiv.org/abs/2007.14062)]
* Reformer: The Efficient Transformer [[paper](https://arxiv.org/abs/2001.04451)]
* Efficient Attentions for Long Document Summarization [[paper](https://arxiv.org/abs/2104.02112)]

### Extract-then-generate

* Pretraining-Based Natural Language Generation for Text Summarization [[paper](https://arxiv.org/abs/1902.09243)]
* Scoring Sentence Singletons and Pairs for Abstractive Summarization [[paper](https://arxiv.org/abs/1906.00077)]
* Neural Extractive Text Summarization with Syntactic Compression [[paper](https://arxiv.org/abs/1902.00863)]
* Long document summarization in a low resource setting using pretrained language models [[paper](https://arxiv.org/abs/2103.00751)]
* Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting [[paper](https://arxiv.org/abs/1805.11080)]
* Summary Level Training of Sentence Rewriting for Abstractive Summarization [[paper](https://arxiv.org/abs/1909.08752)]

### Divide-and-conquer
* A Divide-and-Conquer Approach to the Summarization of Long Documents [[paper](https://arxiv.org/abs/2004.06190)]
* Globalizing BERT-based transformer architectures for long document summarization [[paper](https://aclanthology.org/2021.eacl-main.154/)]

### Hierarchical Models
* A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents [[paper](https://arxiv.org/abs/1804.05685)]
* Hierarchical Learning for Generation with Long Source Sequences [[paper](https://arxiv.org/abs/2104.07545)]
* A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining [[paper](https://arxiv.org/abs/2004.02016)]

## Datasets
### Documents
* GovReport [[paper](https://arxiv.org/abs/2104.02112)]
* ArXiv and PubMed [[paper](https://arxiv.org/abs/1804.05685)]
* BillSum [[paper](https://arxiv.org/abs/1910.00523)]
* BIGPATENT [[paper](https://arxiv.org/abs/1906.03741)]

### Dialogues
* QMSum [[paper](https://arxiv.org/abs/2104.05938)]
* AMI [[paper](https://www.semanticscholar.org/paper/The-AMI-Meeting-Corpus%3A-A-Pre-announcement-Carletta-Ashby/e4e0fd56309e28b28bb47c9a72ad6111c76bb8b9)]
* ICSI [[paper](https://ieeexplore.ieee.org/document/1198793)]
* MediaSum [[paper](https://arxiv.org/abs/2103.06410)]
* SummScreen [[paper](https://arxiv.org/abs/2104.07091)]










